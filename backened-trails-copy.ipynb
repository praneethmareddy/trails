{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74eeabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sample files including master templates created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directories\n",
    "folders = ['ciq_files', 'templates', 'logs', 'master_templates']\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Create CIQ Excel files\n",
    "df_airtel = pd.DataFrame({\n",
    "    'Parameter': ['x1', 'x2', 'x3'],\n",
    "    'Value': [10, 20, 30]\n",
    "})\n",
    "df_jio = pd.DataFrame({\n",
    "    'Parameter': ['y1', 'y2', 'y3'],\n",
    "    'Value': [40, 50, 60]\n",
    "})\n",
    "\n",
    "df_airtel.to_excel('ciq_files/airtel_config.xlsx', index=False)\n",
    "df_jio.to_excel('ciq_files/jio_config.xlsx', index=False)\n",
    "\n",
    "# Create Template files\n",
    "with open('templates/airtel_template.txt', 'w') as f:\n",
    "    f.write(\"@BaseStationA\\nparam1, param2\\n100, 200\\n\")\n",
    "with open('templates/jio_template.txt', 'w') as f:\n",
    "    f.write(\"@BaseStationB\\nparamA, paramB\\n300, 400\\n\")\n",
    "\n",
    "# Create Log files\n",
    "with open('logs/airtel_log.txt', 'w') as f:\n",
    "    f.write(\"2025-05-11 12:00:00 [INFO] Base station configured successfully with parameters x1=10, x2=20.\\n\")\n",
    "with open('logs/jio_log.txt', 'w') as f:\n",
    "    f.write(\"2025-05-11 12:05:00 [INFO] Base station configured with parameters y1=40, y2=50.\\n\")\n",
    "\n",
    "# Create Master Template files\n",
    "with open('master_templates/global_master_template.txt', 'w') as f:\n",
    "    f.write(\"@GlobalTemplate\\ncommon_param1, common_param2\\n999, 888\\n\")\n",
    "\n",
    "with open('master_templates/master_template_airtel.txt', 'w') as f:\n",
    "    f.write(\"@MasterAirtel\\nx1, x2, x3\\n10, 20, 30\\n\")\n",
    "\n",
    "with open('master_templates/master_template_jio.txt', 'w') as f:\n",
    "    f.write(\"@MasterJio\\ny1, y2, y3\\n40, 50, 60\\n\")\n",
    "\n",
    "print(\"All sample files including master templates created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b15b073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709caab518aa4670b0c84b33a69a29a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saip9\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saip9\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd83ea5b746e47fa9554cae700a3db25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f2b76363aa4191948eae904f4c8c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b2fa1d69574f41870ce955ea83ff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a152837c1a14380a6fb5a5a6cebce47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saip9\\AppData\\Local\\Temp\\ipykernel_17768\\956159988.py:26: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.1:8b\")\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas openpyxl sentence-transformers transformers langchain-community langchain-core\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 📁 Paths\n",
    "CIQ_FOLDER = './ciq_files'\n",
    "TEMPLATES_FOLDER = './templates'\n",
    "LOGS_FOLDER = './logs'\n",
    "\n",
    "# 🔧 Models\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model_name = \"gpt2\"  # or use 'distilbert-base-uncased', 'bert-base-uncased', etc.\n",
    "\n",
    "# Load the tokenizer and model directly (no authentication needed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0812da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Embedding Matrix:\n",
      "         10        50      and  basestationa  masterairtel   param1   param2  \\\n",
      "0  0.552816  0.000000  0.00000       0.00000      0.000000  0.00000  0.00000   \n",
      "1  0.000000  0.523381  0.00000       0.00000      0.000000  0.00000  0.00000   \n",
      "2  0.000000  0.000000  0.47633       0.47633      0.000000  0.47633  0.47633   \n",
      "3  0.000000  0.000000  0.00000       0.00000      0.525473  0.00000  0.00000   \n",
      "\n",
      "   parameter     value      with        x1        x2        x3        y2  \n",
      "0   0.435847  0.435847  0.352855  0.435847  0.000000  0.000000  0.000000  \n",
      "1   0.412640  0.412640  0.334067  0.000000  0.000000  0.000000  0.523381  \n",
      "2   0.000000  0.000000  0.304035  0.000000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.000000  0.414289  0.525473  0.525473  0.000000  \n",
      "\n",
      "✅ TF-IDF embeddings computed. Ready for retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saip9\\AppData\\Local\\Temp\\ipykernel_19496\\920751702.py:28: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.1:8b\")\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas openpyxl scikit-learn transformers langchain-community langchain-core\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ❌ Commented out SentenceTransformer embedding logic\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 📁 Paths\n",
    "CIQ_FOLDER = './ciq_files'\n",
    "TEMPLATES_FOLDER = './templates'\n",
    "LOGS_FOLDER = './logs'\n",
    "MASTER_TEMPLATES_FOLDER = './master_templates'  # ✅ Added\n",
    "\n",
    "# 🔧 Tokenizer model\n",
    "model_name = \"gpt2\"  # or use 'distilbert-base-uncased', etc.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 🧠 LLM model via LangChain\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")\n",
    "\n",
    "# ✅ TF-IDF Embedder (replacement for SentenceTransformer)\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Example usage (replace this with your real text data)\n",
    "sample_docs = [\n",
    "    \"Parameter x1 with value 10\", \n",
    "    \"Parameter y2 with value 50\", \n",
    "    \"@BaseStationA with param1 and param2\", \n",
    "    \"@MasterAirtel x1, x2, x3\"\n",
    "]\n",
    "\n",
    "# Fit and transform sample data (for demo)\n",
    "tfidf_matrix = vectorizer.fit_transform(sample_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Print matrix for verification\n",
    "print(\"\\nTF-IDF Embedding Matrix:\")\n",
    "print(pd.DataFrame(dense_matrix, columns=feature_names))\n",
    "\n",
    "print(\"\\n✅ TF-IDF embeddings computed. Ready for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc5a27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/15.0 MB 2.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.3/15.0 MB 2.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 2.3 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.4/15.0 MB 2.4 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.9/15.0 MB 2.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.7/15.0 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.7/15.0 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.2/15.0 MB 2.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.3/15.0 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 7.3/15.0 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.4/15.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.4/15.0 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.5/15.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.5/15.0 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.3/15.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.4/15.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.2/15.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a676ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Chosen Category: template\n",
      "❌ No FAISS index found for category 'template'. Please run preprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# === Folders ===\n",
    "FOLDERS = {\n",
    "    \"ciq\": \"ciq_files\",\n",
    "    \"template\": \"templates\",\n",
    "    \"log\": \"logs\",\n",
    "    \"master_template\": \"master_templates\"\n",
    "}\n",
    "PICKLE_DIR = \"faiss_retriever_pickles\"\n",
    "os.makedirs(PICKLE_DIR, exist_ok=True)\n",
    "\n",
    "# === Individual Loaders ===\n",
    "\n",
    "def load_ciq_documents(folder):\n",
    "    documents = []\n",
    "    for filepath in glob.glob(f\"{folder}/*.xlsx\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            df = pd.read_excel(filepath, sheet_name=None)\n",
    "            content = \"\\n\".join(\n",
    "                f\"Sheet: {sheet}\\n{df[sheet].head(5).to_csv(index=False)}\"\n",
    "                for sheet in df\n",
    "            )\n",
    "            documents.append(Document(\n",
    "                page_content=f\"[{filename}]\\n{content}\",\n",
    "                metadata={\"type\": \"ciq\", \"path\": filepath}\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error reading CIQ {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "def load_template_documents(folder):\n",
    "    return load_generic_text_documents(folder, \"template\")\n",
    "\n",
    "def load_master_template_documents(folder):\n",
    "    return load_generic_text_documents(folder, \"master_template\")\n",
    "\n",
    "def load_log_documents(folder):\n",
    "    return load_generic_text_documents(folder, \"log\")\n",
    "\n",
    "def load_generic_text_documents(folder, doc_type):\n",
    "    documents = []\n",
    "    for filepath in glob.glob(f\"{folder}/*\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                content = f.read()\n",
    "            documents.append(Document(\n",
    "                page_content=f\"[{filename}]\\n{content}\",\n",
    "                metadata={\"type\": doc_type, \"path\": filepath}\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error reading {doc_type} {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# === FAISS Preprocessing ===\n",
    "\n",
    "def create_faiss_index(docs, doc_type):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    doc_embeddings = [embeddings.embed(doc.page_content) for doc in docs]\n",
    "    doc_embeddings_np = np.array(doc_embeddings).astype(\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(doc_embeddings_np.shape[1])\n",
    "    index.add(doc_embeddings_np)\n",
    "\n",
    "    with open(f\"{PICKLE_DIR}/{doc_type}_faiss_index.pkl\", \"wb\") as f:\n",
    "        pickle.dump(index, f)\n",
    "    print(f\"✅ FAISS index created and saved for {doc_type}\")\n",
    "\n",
    "def preprocess_faiss():\n",
    "    ciq_docs = load_ciq_documents(FOLDERS[\"ciq\"])\n",
    "    template_docs = load_template_documents(FOLDERS[\"template\"])\n",
    "    master_template_docs = load_master_template_documents(FOLDERS[\"master_template\"])\n",
    "    log_docs = load_log_documents(FOLDERS[\"log\"])\n",
    "\n",
    "    create_faiss_index(ciq_docs, \"ciq\")\n",
    "    create_faiss_index(template_docs, \"template\")\n",
    "    create_faiss_index(master_template_docs, \"master_template\")\n",
    "    create_faiss_index(log_docs, \"log\")\n",
    "\n",
    "# === LLM Router with Category, Format, and General Fallback ===\n",
    "\n",
    "router_llm = Ollama(model=\"llama3\")\n",
    "\n",
    "def route_db(query):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are a classification assistant for a telecom configuration system. Classify the user query into one of the following categories based on the content and context.\n",
    "\n",
    "    Categories and Descriptions:\n",
    "\n",
    "    1. **ciq**  \n",
    "    - Format: `.xlsx` Excel sheets containing CIQ (Customer Information Questionnaire) files.  \n",
    "    - Keywords: frequency, PCI, TAC, cell ID, etc. Used to generate NE templates.\n",
    "\n",
    "    2. **template**  \n",
    "    - Format: `.txt`, `.cfg` containing NE templates.  \n",
    "    - Operator-specific templates, generated from CIQs.\n",
    "\n",
    "    3. **master_template**  \n",
    "    - Format: `.txt`, `.cfg`.  \n",
    "    - Contains merged or global NE templates.\n",
    "\n",
    "    4. **log**  \n",
    "    - Format: `.txt`, `.log`.  \n",
    "    - Contains system logs, error traces, or deployment diagnostics.\n",
    "\n",
    "    5. **general**  \n",
    "    - Not related to network configuration. Examples: \"how are you\", \"2+2\", etc.\n",
    "\n",
    "    Only return the category label: ciq, template, master_template, log, or general.\n",
    "\n",
    "    Query: \"{query}\"\n",
    "\n",
    "    Category:\n",
    "    \"\"\")\n",
    "\n",
    "    raw_output = router_llm.invoke(prompt.format(query=query)).strip().lower()\n",
    "\n",
    "    # Additional robust fallback\n",
    "    if \"ciq\" in raw_output:\n",
    "        return \"ciq\"\n",
    "    elif \"template\" in raw_output:\n",
    "        return \"template\"\n",
    "    elif \"master_template\" in raw_output:\n",
    "        return \"master_template\"\n",
    "    elif \"log\" in raw_output:\n",
    "        return \"log\"\n",
    "    else:\n",
    "        return \"general\"  # Default fallback for ambiguous queries\n",
    "\n",
    "# === FAISS Query-Time Retrieval ===\n",
    "\n",
    "def load_faiss_index(doc_type):\n",
    "    with open(f\"{PICKLE_DIR}/{doc_type}_faiss_index.pkl\", \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_documents_by_category(category):\n",
    "    if category == \"ciq\":\n",
    "        return load_ciq_documents(FOLDERS[\"ciq\"])\n",
    "    elif category == \"template\":\n",
    "        return load_template_documents(FOLDERS[\"template\"])\n",
    "    elif category == \"master_template\":\n",
    "        return load_master_template_documents(FOLDERS[\"master_template\"])\n",
    "    elif category == \"log\":\n",
    "        return load_log_documents(FOLDERS[\"log\"])\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def retrieve_documents(query, top_k=3):\n",
    "    category = route_db(query)\n",
    "    print(f\"\\n🔍 Chosen Category: {category}\")\n",
    "    \n",
    "    if category == \"general\":\n",
    "        print(\"\\n📝 This query is considered general and doesn't match specific categories.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        faiss_index = load_faiss_index(category)\n",
    "        \n",
    "        # Get the embedding of the query\n",
    "        query_embedding = OpenAIEmbeddings().embed(query)\n",
    "        query_embedding_np = np.array([query_embedding]).astype(\"float32\")\n",
    "        \n",
    "        # Perform the search\n",
    "        _, indices = faiss_index.search(query_embedding_np, top_k)  # Indices of top_k results\n",
    "        \n",
    "        print(f\"\\n🔍 Top {top_k} results from category: {category}\")\n",
    "        for i in range(top_k):\n",
    "            doc_index = indices[0][i]\n",
    "            doc = load_documents_by_category(category)[doc_index]\n",
    "            print(f\"\\nResult #{i+1}:\\nFrom: {doc.metadata['path']}\\n{doc.page_content[:500]}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ No FAISS index found for category '{category}'. Please run preprocessing.\")\n",
    "\n",
    "# === Main Run ===\n",
    "\n",
    "# Uncomment the line below to preprocess and create FAISS indexes for all categories\n",
    "# preprocess_faiss()\n",
    "\n",
    "# Example query\n",
    "user_query = \"show me global master template\"\n",
    "retrieve_documents(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92931210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Routed to: template\n",
      "\n",
      "🔎 Top 3 documents from template:\n",
      "\n",
      "📄 templates\\airtel_template.txt\n",
      "[airtel_template.txt]\n",
      "@BaseStationA\n",
      "param1, param2\n",
      "100, 200\n",
      "\n",
      "\n",
      "📄 templates\\airtel_template.csv\n",
      "[airtel_template.csv]\n",
      "@BaseStationA\n",
      "param1, param2\n",
      "100, 200\n",
      "\n",
      "\n",
      "📄 templates\\jio_template.txt\n",
      "[jio_template.txt]\n",
      "@BaseStationB\n",
      "paramA, paramB\n",
      "300, 400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# === Folders ===\n",
    "FOLDERS = {\n",
    "    \"ciq\": \"ciq_files\",\n",
    "    \"template\": \"templates\",\n",
    "    \"log\": \"logs\",\n",
    "    \"master_template\": \"master_templates\"\n",
    "}\n",
    "FAISS_INDEX_DIR = \"faiss_indexes\"\n",
    "os.makedirs(FAISS_INDEX_DIR, exist_ok=True)\n",
    "\n",
    "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "router_llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# === Loaders ===\n",
    "def load_generic_text_documents(folder, doc_type):\n",
    "    documents = []\n",
    "    for filepath in glob.glob(f\"{folder}/*\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                content = f.read()\n",
    "            documents.append(Document(\n",
    "                page_content=f\"[{filename}]\\n{content}\",\n",
    "                metadata={\"type\": doc_type, \"path\": filepath}\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error reading {doc_type} {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "def load_ciq_documents(folder):\n",
    "    documents = []\n",
    "    for filepath in glob.glob(f\"{folder}/*.xlsx\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            df = pd.read_excel(filepath, sheet_name=None)\n",
    "            content = \"\\n\".join(\n",
    "                f\"Sheet: {sheet}\\n{df[sheet].head(5).to_csv(index=False)}\"\n",
    "                for sheet in df\n",
    "            )\n",
    "            documents.append(Document(\n",
    "                page_content=f\"[{filename}]\\n{content}\",\n",
    "                metadata={\"type\": \"ciq\", \"path\": filepath}\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error reading CIQ {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# === Embedding & FAISS ===\n",
    "def build_faiss_index(docs: List[Document], doc_type: str):\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "    embeddings = EMBED_MODEL.encode(texts, convert_to_numpy=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    with open(f\"{FAISS_INDEX_DIR}/{doc_type}_docs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "    faiss.write_index(index, f\"{FAISS_INDEX_DIR}/{doc_type}_index.faiss\")\n",
    "    print(f\"✅ Built FAISS index for {doc_type} ({len(docs)} docs)\")\n",
    "\n",
    "def preprocess_all_faiss():\n",
    "    build_faiss_index(load_ciq_documents(FOLDERS[\"ciq\"]), \"ciq\")\n",
    "    build_faiss_index(load_generic_text_documents(FOLDERS[\"template\"], \"template\"), \"template\")\n",
    "    build_faiss_index(load_generic_text_documents(FOLDERS[\"log\"], \"log\"), \"log\")\n",
    "    build_faiss_index(load_generic_text_documents(FOLDERS[\"master_template\"], \"master_template\"), \"master_template\")\n",
    "\n",
    "def load_faiss_index(doc_type):\n",
    "    index = faiss.read_index(f\"{FAISS_INDEX_DIR}/{doc_type}_index.faiss\")\n",
    "    with open(f\"{FAISS_INDEX_DIR}/{doc_type}_docs.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "    return index, docs\n",
    "\n",
    "# === Advanced Router with Semantic & Keyword Backup ===\n",
    "def route_db(query):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant in a telecom system. Based on the query, choose **only one** of the following labels:\n",
    "- ciq\n",
    "- template\n",
    "- master_template\n",
    "- log\n",
    "- general\n",
    "\n",
    "Descriptions:\n",
    "1. ciq: .xlsx input files with frequency, PCI, TAC, etc.\n",
    "2. template: .txt/.cfg NE configs generated from CIQs.\n",
    "3. master_template: unified/merged/global templates.\n",
    "4. log: deployment, error or trace logs.\n",
    "5. general: chit-chat, math, or unrelated.\n",
    "\n",
    "Classify this:\n",
    "Query: \"{query}\"\n",
    "Only respond with one label from above.\n",
    "Category:\n",
    "    \"\"\")\n",
    "    \n",
    "    raw = router_llm.invoke(prompt.format(query=query)).strip().lower()\n",
    "    categories = [\"ciq\", \"template\", \"master_template\", \"log\", \"general\"]\n",
    "    for cat in categories:\n",
    "        if raw == cat:\n",
    "            return cat\n",
    "\n",
    "    # Fallback: keyword based\n",
    "    fallback_keywords = {\n",
    "        \"master_template\": [\"global master\", \"unified\", \"merged config\", \"master_template\"],\n",
    "        \"template\": [\"@section\", \"parameter\", \"value\", \"template\"],\n",
    "        \"ciq\": [\"pci\", \"tac\", \"band\", \"earfcn\", \".xlsx\", \"ciq\"],\n",
    "        \"log\": [\"error\", \"fail\", \"log\", \".log\", \"trace\"],\n",
    "    }\n",
    "    query_lc = query.lower()\n",
    "    for cat, keywords in fallback_keywords.items():\n",
    "        if any(k in query_lc for k in keywords):\n",
    "            return cat\n",
    "\n",
    "    return \"general\"\n",
    "\n",
    "# === Query Handler ===\n",
    "def retrieve_documents(query, top_k=3):\n",
    "    category = route_db(query)\n",
    "    print(f\"\\n🔍 Routed to: {category}\")\n",
    "\n",
    "    if category == \"general\":\n",
    "        print(\"📝 General query detected. No retrieval performed.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        index, docs = load_faiss_index(category)\n",
    "        query_vec = EMBED_MODEL.encode([query])\n",
    "        distances, indices = index.search(query_vec, top_k)\n",
    "        print(f\"\\n🔎 Top {top_k} documents from {category}:\")\n",
    "        for i in indices[0]:\n",
    "            doc = docs[i]\n",
    "            print(f\"\\n📄 {doc.metadata['path']}\\n{doc.page_content[:500]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading index for {category}: {e}\")\n",
    "\n",
    "# === Run ===\n",
    "# Uncomment this once to build the FAISS index\n",
    "# preprocess_all_faiss()\n",
    "\n",
    "# Example usage\n",
    "retrieve_documents(\"i want to generate ne template for airtel where i want fo have param1 value as 104 in basestation1 section\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84ac7e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ciq docs from: ciq_files\n",
      "  Reading: ciq_files\\airtel_config.xlsx\n",
      "  Reading: ciq_files\\jio_config.xlsx\n",
      "Loaded 2 ciq docs\n",
      "\n",
      "Loading template docs from: templates\n",
      "  Reading: templates\\airtel_template.txt\n",
      "  Reading: templates\\jio_template.txt\n",
      "Loaded 2 template docs\n",
      "\n",
      "Loading log docs from: logs\n",
      "  Reading: logs\\airtel_log.txt\n",
      "  Reading: logs\\jio_log.txt\n",
      "Loaded 2 log docs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# === Settings ===\n",
    "CIQ_FOLDER = \"ciq_files\"\n",
    "TEMPLATES_FOLDER = \"templates\"\n",
    "LOGS_FOLDER = \"logs\"\n",
    "\n",
    "# === Embedding model ===\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === Helper: Load and wrap docs ===\n",
    "def load_docs(folder, doc_type):\n",
    "    docs = []\n",
    "    print(f\"Loading {doc_type} docs from: {folder}\")\n",
    "    for file in glob.glob(f\"{folder}/*\"):\n",
    "        print(f\"  Reading: {file}\")\n",
    "        try:\n",
    "            if file.endswith(\".xlsx\") and doc_type == \"ciq\":\n",
    "                df = pd.read_excel(file, sheet_name=None)\n",
    "                text = \"\\n\".join(\n",
    "                    f\"Sheet: {sheet}\\n{df[sheet].head(5).to_csv(index=False)}\"\n",
    "                    for sheet in df\n",
    "                )\n",
    "            else:\n",
    "                with open(file, \"r\") as f:\n",
    "                    text = f.read()\n",
    "            docs.append(Document(page_content=text, metadata={\"type\": doc_type, \"path\": file}))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Failed to read {file}: {e}\")\n",
    "    print(f\"Loaded {len(docs)} {doc_type} docs\\n\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# === Load documents ===\n",
    "ciq_docs = load_docs(CIQ_FOLDER, \"ciq\")\n",
    "template_docs = load_docs(TEMPLATES_FOLDER, \"template\")\n",
    "log_docs = load_docs(LOGS_FOLDER, \"log\")\n",
    "\n",
    "# === Store each in separate FAISS DB ===\n",
    "db_ciq = FAISS.from_documents(ciq_docs, embedder)\n",
    "db_template = FAISS.from_documents(template_docs, embedder)\n",
    "db_log = FAISS.from_documents(log_docs, embedder)\n",
    "\n",
    "# === Routing Agent ===\n",
    "router_llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "def route_db(query):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Given the query below, classify it into one of the following categories:\n",
    "- ciq: Related to Excel/CIQ network config\n",
    "- template: Related to network configuration templates\n",
    "- log: Related to logs or error/debug info\n",
    "\n",
    "Query: \"{query}\"\n",
    "Category:\"\"\")\n",
    "    category = router_llm.invoke(prompt.format(query=query)).content.strip().lower()\n",
    "    if \"ciq\" in category:\n",
    "        return db_ciq\n",
    "    elif \"template\" in category:\n",
    "        return db_template\n",
    "    elif \"log\" in category:\n",
    "        return db_log\n",
    "    else:\n",
    "        return db_template  # fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "296508ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
      "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \n",
      "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8451d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Processing Query: give me jio log\n",
      "\n",
      "🔍 Routed to: log\n",
      "\n",
      "✅ Selected file: logs\\jio_log.txt\n",
      "\n",
      "🧠 Response:\n",
      " Based on the provided log context, it appears that a base station has been successfully configured with certain parameters.\n",
      "\n",
      "Here's what we can infer from the log:\n",
      "\n",
      "* The configuration process was logged as \"INFO\", indicating a normal operation or status update.\n",
      "* Two parameters, y1 and y2, were specified during the configuration. Values of 40 and 50, respectively, have been assigned to them.\n",
      "* This log entry is dated May 11th, 2025, at 12:05 PM.\n",
      "\n",
      "Considering this information, I would recommend verifying if these parameter settings are in line with any predefined or required configurations for the base station. It might also be useful to check if there were any subsequent logs or operations that followed this configuration update.\n",
      "\n",
      "💬 Memory Content:\n",
      "[HumanMessage(content='give me jio log', additional_kwargs={}, response_metadata={}), AIMessage(content='Based on the provided log context, it appears that a base station has been successfully configured with certain parameters.\\n\\nHere\\'s what we can infer from the log:\\n\\n* The configuration process was logged as \"INFO\", indicating a normal operation or status update.\\n* Two parameters, y1 and y2, were specified during the configuration. Values of 40 and 50, respectively, have been assigned to them.\\n* This log entry is dated May 11th, 2025, at 12:05 PM.\\n\\nConsidering this information, I would recommend verifying if these parameter settings are in line with any predefined or required configurations for the base station. It might also be useful to check if there were any subsequent logs or operations that followed this configuration update.', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "🔄 Processing Query: what are the y1 and y2 values?\n",
      "\n",
      "🔍 Routed to: general\n",
      "📝 General query. No context retrieval.\n",
      "\n",
      "💬 Memory Content:\n",
      "[HumanMessage(content='give me jio log', additional_kwargs={}, response_metadata={}), AIMessage(content='Based on the provided log context, it appears that a base station has been successfully configured with certain parameters.\\n\\nHere\\'s what we can infer from the log:\\n\\n* The configuration process was logged as \"INFO\", indicating a normal operation or status update.\\n* Two parameters, y1 and y2, were specified during the configuration. Values of 40 and 50, respectively, have been assigned to them.\\n* This log entry is dated May 11th, 2025, at 12:05 PM.\\n\\nConsidering this information, I would recommend verifying if these parameter settings are in line with any predefined or required configurations for the base station. It might also be useful to check if there were any subsequent logs or operations that followed this configuration update.', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "🔄 Processing Query: update the above log y1 to 10 and y2 to 20 and give\n",
      "\n",
      "🔍 Routed to: log\n",
      "❌ Retrieval or LLM failed: 'HumanMessage' object is not subscriptable\n",
      "\n",
      "💬 Memory Content:\n",
      "[HumanMessage(content='give me jio log', additional_kwargs={}, response_metadata={}), AIMessage(content='Based on the provided log context, it appears that a base station has been successfully configured with certain parameters.\\n\\nHere\\'s what we can infer from the log:\\n\\n* The configuration process was logged as \"INFO\", indicating a normal operation or status update.\\n* Two parameters, y1 and y2, were specified during the configuration. Values of 40 and 50, respectively, have been assigned to them.\\n* This log entry is dated May 11th, 2025, at 12:05 PM.\\n\\nConsidering this information, I would recommend verifying if these parameter settings are in line with any predefined or required configurations for the base station. It might also be useful to check if there were any subsequent logs or operations that followed this configuration update.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Paths & Setup ===\n",
    "FOLDERS = {\n",
    "    \"ciq\": \"ciq_files\",\n",
    "    \"template\": \"templates\",\n",
    "    \"log\": \"logs\",\n",
    "    \"master_template\": \"master_templates\"\n",
    "}\n",
    "FAISS_INDEX_DIR = \"faiss_indexes\"\n",
    "os.makedirs(FAISS_INDEX_DIR, exist_ok=True)\n",
    "\n",
    "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "router_llm = Ollama(model=\"llama3\")\n",
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "# === Loaders ===\n",
    "def load_generic_text_documents(folder, doc_type):\n",
    "    documents = []\n",
    "    for filepath in glob.glob(f\"{folder}/*\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                content = f.read()\n",
    "            documents.append(Document(\n",
    "                page_content=f\"[{filename}]\\n{content}\",\n",
    "                metadata={\"type\": doc_type, \"path\": filepath}\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error reading {doc_type} {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "def load_ciq_documents(folder):\n",
    "    documents = []\n",
    "    for filepath in glob.glob(f\"{folder}/*.xlsx\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            df = pd.read_excel(filepath, sheet_name=None)\n",
    "            content = \"\\n\".join(\n",
    "                f\"Sheet: {sheet}\\n{df[sheet].head(5).to_csv(index=False)}\"\n",
    "                for sheet in df\n",
    "            )\n",
    "            documents.append(Document(\n",
    "                page_content=f\"[{filename}]\\n{content}\",\n",
    "                metadata={\"type\": \"ciq\", \"path\": filepath}\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error reading CIQ {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# === FAISS Embedding Indexing ===\n",
    "def build_faiss_index(docs: List[Document], doc_type: str):\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "    embeddings = EMBED_MODEL.encode(texts, convert_to_numpy=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    with open(f\"{FAISS_INDEX_DIR}/{doc_type}_docs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "    faiss.write_index(index, f\"{FAISS_INDEX_DIR}/{doc_type}_index.faiss\")\n",
    "    print(f\"✅ Built FAISS index for {doc_type} ({len(docs)} docs)\")\n",
    "\n",
    "def preprocess_all_faiss():\n",
    "    build_faiss_index(load_ciq_documents(FOLDERS[\"ciq\"]), \"ciq\")\n",
    "    build_faiss_index(load_generic_text_documents(FOLDERS[\"template\"], \"template\"), \"template\")\n",
    "    build_faiss_index(load_generic_text_documents(FOLDERS[\"log\"], \"log\"), \"log\")\n",
    "    build_faiss_index(load_generic_text_documents(FOLDERS[\"master_template\"], \"master_template\"), \"master_template\")\n",
    "\n",
    "def load_faiss_index(doc_type):\n",
    "    index = faiss.read_index(f\"{FAISS_INDEX_DIR}/{doc_type}_index.faiss\")\n",
    "    with open(f\"{FAISS_INDEX_DIR}/{doc_type}_docs.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "    return index, docs\n",
    "\n",
    "# === Router ===\n",
    "def route_db(query):\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Classify the user query into one of the categories:\n",
    "\n",
    "    - ciq: Excel CIQ files with columns like PCI, TAC, etc.\n",
    "    - template: Config templates generated from CIQ.\n",
    "    - master_template: Global/merged/unified templates.\n",
    "    - log: Diagnostic log files or error traces.\n",
    "    - general: Chat or unrelated to files.\n",
    "\n",
    "    Query: \"{query}\"\n",
    "    Only respond with one of: ciq, template, master_template, log, general.\n",
    "    Category:\n",
    "    \"\"\")\n",
    "    raw = router_llm.invoke(prompt.format(query=query)).strip().lower()\n",
    "    return raw if raw in [\"ciq\", \"template\", \"master_template\", \"log\"] else \"general\"\n",
    "\n",
    "# === Prompts per Category ===\n",
    "ciq_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a telecom assistant. Use the CIQ (Customer Information Questionnaire) sheet data below to answer the user query.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\n",
    "Answer in a structured and helpful way:\n",
    "\"\"\")\n",
    "\n",
    "template_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a config assistant. Use the following NE template (generated from a CIQ) to answer the question.\n",
    "\n",
    "Template Context:\n",
    "{context}\n",
    "\n",
    "Query:\n",
    "{query}\n",
    "\n",
    "Response:\n",
    "\"\"\")\n",
    "\n",
    "master_template_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a deployment assistant. The context below contains a **global or merged NE master template**. Use it to answer the query.\n",
    "\n",
    "Master Template:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\n",
    "Answer with high-level clarity:\n",
    "\"\"\")\n",
    "\n",
    "log_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a diagnostics assistant. The context contains a system or error log. Use it to troubleshoot or respond.\n",
    "\n",
    "Log Context:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\n",
    "Provide an insightful and actionable answer:\n",
    "\"\"\")\n",
    "\n",
    "# === Memory (Chat History) ===\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "# === Query Execution ===\n",
    "def retrieve_and_respond(query, top_k=1):\n",
    "    category = route_db(query)\n",
    "    print(f\"\\n🔍 Routed to: {category}\")\n",
    "\n",
    "    if category == \"general\":\n",
    "        print(\"📝 General query. No context retrieval.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        index, docs = load_faiss_index(category)\n",
    "        query_vec = EMBED_MODEL.encode([query])\n",
    "        distances, indices = index.search(query_vec, top_k)\n",
    "        selected_doc = docs[indices[0][0]] if indices[0].size > 0 else None\n",
    "        context = selected_doc.page_content if selected_doc else \"No relevant context found.\"\n",
    "\n",
    "        # Add previous conversation from memory\n",
    "        conversation_history = \"\\n\".join([f\"User: {msg['input']}\\nAssistant: {msg['output']}\" for msg in memory.buffer])\n",
    "\n",
    "        # Modify the prompt to include both the current context and conversation history\n",
    "        if category == \"ciq\":\n",
    "            prompt = ciq_prompt\n",
    "        elif category == \"template\":\n",
    "            prompt = template_prompt\n",
    "        elif category == \"master_template\":\n",
    "            prompt = master_template_prompt\n",
    "        elif category == \"log\":\n",
    "            prompt = log_prompt\n",
    "        else:\n",
    "            prompt = PromptTemplate.from_template(\"Context:\\n{context}\\nQuery:\\n{query}\\nAnswer:\")\n",
    "\n",
    "        # Include memory (previous conversation) in the prompt\n",
    "        prompt_with_memory = prompt.format(context=f\"{conversation_history}\\n{context}\", query=query)\n",
    "        # Run LLM with memory context\n",
    "        response = llm.invoke(prompt_with_memory)\n",
    "        memory.save_context({\"input\": query}, {\"output\": response})\n",
    "\n",
    "        # Output\n",
    "        print(f\"\\n✅ Selected file: {selected_doc.metadata['path'] if selected_doc else 'None'}\")\n",
    "        print(\"\\n🧠 Response:\\n\", response)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Retrieval or LLM failed: {e}\")\n",
    "\n",
    "# === Build indexes once ===\n",
    "# preprocess_all_faiss()\n",
    "\n",
    "# === Run a Query ===\n",
    "# retrieve_and_respond(\"give me jio log\")\n",
    "# === Run Multiple Queries to Check Memory ===\n",
    "def test_memory():\n",
    "    queries = [\n",
    "        \"give me jio log\",\n",
    "        \"what are the y1 and y2 values?\",\n",
    "        \"update the above log y1 to 10 and y2 to 20 and give\",\n",
    "        # \"is there any issue with the ciq data?\",\n",
    "        # \"give me the troubleshooting steps from the log\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\n🔄 Processing Query: {query}\")\n",
    "        retrieve_and_respond(query)\n",
    "        print(\"\\n💬 Memory Content:\")\n",
    "        print(memory.buffer)  # Show the conversation memory buffer\n",
    "\n",
    "# Run the test to check memory\n",
    "test_memory()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Selected source: log\n",
      "\n",
      "🧠 Response:\n",
      " 2025-05-11 12:05:00 [INFO] Base station configured with parameters y1=40, y2=50.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Run the system ===\n",
    "query = \"give me jio_template\"\n",
    "selected_db = route_db(query)\n",
    "\n",
    "# Retrieve docs from selected vector DB\n",
    "results = selected_db.similarity_search(query, k=1)\n",
    "context = results[0].page_content if results else \"No relevant context found.\"\n",
    "\n",
    "# === Final Prompt ===\n",
    "final_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an intelligent assistant. Use the context to answer the query.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Answer:\"\"\")\n",
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "response = llm.invoke(final_prompt.format(context=context, query=query))\n",
    "\n",
    "# === Output ===\n",
    "print(\"\\n✅ Selected source:\", results[0].metadata[\"type\"] if results else \"None\")\n",
    "print(\"\\n🧠 Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dea9aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  ID              SIZE      MODIFIED     \n",
      "llama3.1:8b           46e0c10c039e    4.9 GB    5 months ago    \n",
      "qwen2.5-coder:1.5b    6d3abb8d2d53    986 MB    5 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbb892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in c:\\programdata\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.59-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\programdata\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.7.0-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.24 (from langchain-community)\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\saip9\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core) (2.8.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\saip9\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\saip9\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.24->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\saip9\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 20.6 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.59-py3-none-any.whl (437 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: typing_extensions, sympy, safetensors, httpx-sse, typing-inspection, torch, huggingface-hub, tokenizers, transformers, pydantic-settings, langsmith, sentence-transformers, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "Successfully installed httpx-sse-0.4.0 huggingface-hub-0.31.1 langchain-0.3.25 langchain-community-0.3.23 langchain-core-0.3.59 langchain-text-splitters-0.3.8 langsmith-0.3.42 pydantic-settings-2.9.1 safetensors-0.5.3 sentence-transformers-4.1.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 transformers-4.51.3 typing-inspection-0.4.0 typing_extensions-4.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\saip9\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\saip9\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\saip9\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\saip9\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas openpyxl sentence-transformers transformers langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a313af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
